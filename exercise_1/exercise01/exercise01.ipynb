{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 1: Introduction 🤖\n",
    "\n",
    "> Welcome to the Advanced Robot Learning and Decision Making exercises!\n",
    "\n",
    " This first exercise will guide you through concepts and tools that we will use throughout the course. By the end of this exercise, you will have a foundational understanding of:\n",
    "\n",
    "1. **Vector Computations in Python (NumPy)**\n",
    "\n",
    "   A brief introduction to NumPy for handling vector and matrix computations, which are fundamental for scientific computations.\n",
    "\n",
    "2. **Introduction to Gymnasium environments** \n",
    "\n",
    "   We will explore the Gymnasium API, a standard API for control and reinforcement learning environments. Gymnasium enables us to develop controllers that can be used with a variety of simulations.\n",
    "\n",
    "3. **Introduction to the Drone Simulation and Environment**\n",
    "\n",
    "   We will introduce you to the Drone environment, which we will use throughout the exercise. The drone simulation already implements input constraints that you would experience on hardware.\n",
    "\n",
    "4. **Design of a simple PD controller for the Drone**\n",
    "\n",
    "   We will implement a simple PD controller that lets the drone hover at a target position.\n",
    "\n",
    "5. **Testing and Submission-System**\n",
    "\n",
    "   Finally, an example task to get familiar with the tests and submission system."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Introduction to NumPy\n",
    "\n",
    "[NumPy](https://numpy.org) is a powerful library for numerical scientific computations in Python. It provides support for large, multi-dimensional arrays and matrices, along with a collection of mathematical functions to operate on these data structures efficiently.\n",
    "\n",
    "**It is important that you understand the concepts of NumPy conveyed in this exercise, as we will use NumPy heavily in the coming exercises.** Feel free to play around with the NumPy functions in this notebook. Many simulation in robotics are written in NumPy, or its brothers [PyTorch](https://pytorch.org) and [JAX](https://docs.jax.dev) - more on that later.\n",
    "\n",
    "### 1.1 Why Use NumPy?\n",
    "\n",
    "#### 1.1.1 Vector Operations\n",
    "\n",
    "Using Python lists for mathematical operations requires explicit loops, which can be cumbersome and error-prone. NumPy allows for element-wise operations without the need for explicit iteration. If you want to implement a simple vector addition\n",
    "\n",
    "$$\n",
    "\\mathbf{a} + \\mathbf{b} =\n",
    "\\begin{pmatrix}\n",
    "a_1 \\\\ a_2 \\\\ a_3\n",
    "\\end{pmatrix} +\n",
    "\\begin{pmatrix}\n",
    "b_1 \\\\ b_2 \\\\ b_3\n",
    "\\end{pmatrix} =\n",
    "\\begin{pmatrix}\n",
    "a_1 + b_1 \\\\ a_2 + b_2 \\\\ a_3 + b_3\n",
    "\\end{pmatrix}\n",
    "$$\n",
    "\n",
    "with Python lists, you would need to loop over each element. NumPy on the other hand allows for element-wise operations without the need for explicit iteration.\n",
    "\n",
    "The benefits of using numpy's vectorized computations compared to standard python operations is enormous!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# magic-commands to enable auto-reloading of external modules\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "# imports\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using Python lists\n",
    "a = [1, 2, 3]\n",
    "b = [4, 5, 6]\n",
    "c_list = [a[i] + b[i] for i in range(len(a))]  # Explicit loop required\n",
    "\n",
    "# Using NumPy\n",
    "a = np.array([1, 2, 3])\n",
    "b = np.array([4, 5, 6])\n",
    "c_numpy = a + b  # No loop, but vectorized computation. Faster for large arrays and more readable\n",
    "\n",
    "# Both results are the same\n",
    "assert np.allclose(c_numpy, c_list), \"The results are not the same\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.1.2 Performance\n",
    "\n",
    "NumPy operations are implemented in C and optimized for performance. This makes them significantly faster than equivalent operations using Python lists. Below you can find an example that produces the same results with Python and NumPy, but will run up to 50x faster in NumPy depending on your machine. Of course, this example is rather trivial. In more complex scenarios, NumPy can accelerate your code by several magnitudes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "a = list(range(int(1e6)))\n",
    "b = list(range(int(1e6)))\n",
    "start = time.perf_counter()\n",
    "c = [a[i] + b[i] for i in range(len(a))]  # loop\n",
    "end = time.perf_counter()\n",
    "duration_list = end - start\n",
    "print(f\"Python list addition time: {duration_list:.2e} seconds\")\n",
    "\n",
    "# NumPy equivalent\n",
    "an = np.array(a)\n",
    "bn = np.array(b)\n",
    "start = time.perf_counter()\n",
    "cn = an + bn  # vectorized\n",
    "end = time.perf_counter()\n",
    "duration_numpy = end - start\n",
    "print(f\"NumPy addition time: {duration_numpy:.2e} seconds\")\n",
    "\n",
    "print(f\"NumPy speed up: {duration_list / duration_numpy:.2f}x\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.1.3 Basic NumPy Operations\n",
    "\n",
    "NumPy includes many vectorized vector, matrix and other linear algebra operations out of the box. You can find them [in the numpy docs](https://numpy.org/doc/stable/reference/routines.html). Have a quick looks at the docs to get a feeling of available functions. To give you some examples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.array([1, 2, 3])\n",
    "y = np.array([4, 5, 6])\n",
    "\n",
    "# Element-wise addition\n",
    "z = x + y\n",
    "print(f\"Vector addition: {z}\")\n",
    "\n",
    "# Element-wise multiplication\n",
    "z = x * y\n",
    "print(f\"Element-wise multiplication: {z}\")\n",
    "\n",
    "# Dot product\n",
    "z = np.dot(x, y)\n",
    "print(f\"Dot product: {z}\")\n",
    "\n",
    "# Matrix multiplication\n",
    "A = np.array([[1, 2], [3, 4]])\n",
    "B = np.array([[5, 6], [7, 8]])\n",
    "z = np.dot(A, B)  # Or A @ B\n",
    "print(f\"Matrix multiplication: \\n{z}\")\n",
    "\n",
    "# Transpose\n",
    "print(f\"A: \\n{A}\\nA.T: \\n{A.T}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Broadcasting in NumPy\n",
    "\n",
    "[Broadcasting](https://numpy.org/doc/stable/user/basics.broadcasting.html) is a powerful feature in NumPy that allows operations between arrays of different shapes by automatically expanding them to a compatible shape.\n",
    "\n",
    "**Rules of Broadcasting**\n",
    "\n",
    "1. If the arrays have different dimensions, the smaller array is padded with leading dimensions of size 1.\n",
    "2. If the shape of the arrays does not match in any dimension, the array with size 1 in that dimension is stretched to match the other array.\n",
    "3. If the dimensions are incompatible (i.e., neither is 1 and they are different), NumPy throws an error.\n",
    "\n",
    "**Why is Broadcasting Useful?**\n",
    "\n",
    "Broadcasting allows for efficient vectorized operations, eliminating the need for explicit loops and improving performance.\n",
    "\n",
    "Examples:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Broadcasting across all elements\n",
    "A = np.array([[1, 2], [3, 4]])\n",
    "b = 5\n",
    "print(f\"A + b: \\n{A + b}\")  # Each element of A is incremented by 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Broadcasting across rows\n",
    "A = np.array([[1, 2], [3, 4]])\n",
    "b = np.array([5, 6])\n",
    "print(f\"A + b: \\n{A + b}\")  # Broadcasting across rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A = np.array([[1, 2], [3, 4]])\n",
    "b = np.array([[5, 6]])\n",
    "print(f\"A + b.T: \\n{A + b.T}\")  # Broadcasting across columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Of course, these examples only scratch the surface of what NumPy can do. If you want to find out more, you can look up details of specific functions and much more in their [documentation](https://numpy.org/doc/stable/index.html).\n",
    "\n",
    "It is good to know that brother libraries such as [PyTorch](https://pytorch.org) and [JAX](https://docs.jax.dev) often have very similar syntax to NumPy!\n",
    "\n",
    "Equipped with some basic knowledge on how to handle linear algebra operations, we can now take a look at the de-facto standard API for learning and control with Python.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. The Gymnasium API"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following is quite some text, but it allows you to gain a background understanding of the framework we use.\n",
    "\n",
    "### 2.1 Background\n",
    "\n",
    "The core idea behind Gymnasium is to provide an [API](https://en.wikipedia.org/wiki/API) for all (single agent)reinforcement learning environments. It also provides a set of (rather simple) environments out of the box that are typically used for benchmarking new algorithms: cartpole, pendulum, mujoco (walker, humanoid, ...), atari, and more. The Gymnasium API is de facto becomgin the standard for projects that are used with control algorithms. Often, Gymnasium environments function as an interface (API) between a lower-level simulation and the user-implemented controller. This is, for instance, the case for MuJoCo environments, such as [Gymnasium Robotics](https://robotics.farama.org/index.html) where the simulation is done using [MuJoCo](https://mujoco.org/), or custom environments, such as our [Crazyflow environment](https://github.com/utiasDSL/crazyflow) where simulation is done using [JAX](https://docs.jax.dev/en/latest/quickstart.html) and [MJX](https://mujoco.readthedocs.io/en/stable/mjx.html), and many more. The Gymnasium Environments implement task-specific details, such as reward functions and termination conditions.\n",
    "\n",
    "At the core of Gymnasium is [`Env`](https://gymnasium.farama.org/api/env/#gymnasium.Env), a high-level python class representing a [markov decision process (MDP)](https://spinningup.openai.com/en/latest/spinningup/rl_intro.html#optional-formalism) from reinforcement learning theory. The class provides users the ability to generate an initial state, transition to new states given an action, and visualize the environment. Alongside Env, [Wrappers](https://gymnasium.farama.org/api/wrappers/#gymnasium.Wrapper) are provided to help augment and modify the environment for particular tasks, for instance the agent observations, rewards and actions taken.\n",
    "\n",
    ">**Note** While Gymnasium was originally developed for reinforcement learning, we will use the same interface for other control implementations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [2.2 Basic Usage](https://gymnasium.farama.org/introduction/basic_usage/#basic-usage)\n",
    "\n",
    "Initializing environments is very easy in Gymnasium and can be done via the [`make()`](https://gymnasium.farama.org/api/registry/#gymnasium.make) function:\n",
    "\n",
    "```python\n",
    "import gymnasium as gym\n",
    "env = gym.make('CartPole-v1')\n",
    "```\n",
    "This function will return an Env for users to interact with (in that case the env with the name `CartPole-v1`). Furthermore, `make()` can provid a number of additional arguments for specifying keywords to the environment, adding more or less wrappers, etc. You will see examples of that later.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The classic “agent-environment loop” pictured below is a simplified representation of how an agent and environment interact with each other. The agent receives an observation about the environment, the agent then selects an action (usually using a controller), which the environment uses to determine the reward and the next observation. The cycle then repeats itself until the environment ends.\n",
    "\n",
    "# ![Agent-Environment Loop](./img/AE_loop_dark.png)\n",
    "\n",
    "\n",
    "For Gymnasium, the “agent-environment-loop” is implemented below.\n",
    "\n",
    "```python\n",
    "import gymnasium as gym\n",
    "\n",
    "env = gym.make(\"LunarLander-v3\", render_mode=\"human\")\n",
    "observation, info = env.reset() # get initial observation\n",
    "\n",
    "episode_over = False\n",
    "while not episode_over:\n",
    "    action = env.action_space.sample()  # randomly sample an action; in practise one would use a controller here\n",
    "    observation, reward, terminated, truncated, info = env.step(action) # step the environment one timestep while applying the action, and return the environment information\n",
    "\n",
    "    episode_over = terminated or truncated\n",
    "    # if one wants to continue to step through the environment when episode_over == True, one must call env.reset()\n",
    "\n",
    "env.close()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After initializing the environment, we **must** call [`env.reset()`](https://gymnasium.farama.org/api/env/#gymnasium.Env.reset) to get the first observation of the environment along with optional additional information. For initializing the environment with a particular random seed or options you can pass those parameters to `reset()` - please refer to the documentation.\n",
    "\n",
    "Next, the agent performs an action in the environment, [`env.step()`](https://gymnasium.farama.org/api/env/#gymnasium.Env.step) executes the selected action (in this case random with `env.action_space.sample()`) to update the environment. This action can be imagined as moving a robot or pressing a button on a games’ controller that causes a change within the environment. As a result, the agent receives a new observation from the updated environment along with a reward for taking the action. This reward could be for instance positive for destroying an enemy or a negative reward for moving into lava. One such action-observation exchange is referred to as a timestep.\n",
    "\n",
    "After some timesteps, the environment may end, this is called the **terminal state**. For instance, the robot may have crashed, or may have succeeded in completing a task, the environment will need to stop as the agent cannot continue. In Gymnasium, if the environment has **terminated**, this is returned by `step()` as the third variable. Similarly, we may also want the environment to end after a fixed number of timesteps, in this case, the environment issues a **truncated** signal. If either of terminated or truncated are True then we end the episode but **in most cases users might wish to restart the environment, this can be done with `env.reset()`**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**There are some important environment design factors that we need you to understand for the following exercises**:\n",
    "- [Observation space](https://gymnasium.farama.org/api/env/#gymnasium.Env.observation_space): All valid observations are contained in that space. For control applications, we most commonly use [Box](https://gymnasium.farama.org/api/spaces/fundamental/#gymnasium.spaces.Box).\n",
    "- [Action space](https://gymnasium.farama.org/api/env/#gymnasium.Env.action_space): All valid actions must be in that space. For control applications, again [Box](https://gymnasium.farama.org/api/spaces/fundamental/#gymnasium.spaces.Box) is most commonly used, or [Discrete](https://gymnasium.farama.org/api/spaces/fundamental/#gymnasium.spaces.Discrete).\n",
    "- **Truncate**: Most environments are set to have a time limit of a certain number of timesteps, which is usually implemented using the [TimeLimit wrapper](https://gymnasium.farama.org/api/wrappers/misc_wrappers/#gymnasium.wrappers.TimeLimit). The environment will truncate if the time limit is reached.\n",
    "- **Terminate**: Environments terminate after reaching a termination condition, such as a drone crashing or reaching its final goal.\n",
    "- If any of truncate or terminate occures, it is returned by the [step function](https://gymnasium.farama.org/api/env/#gymnasium.Env.step) as shown above. In that case you usually reset the environment.\n",
    "- **Reset**: Environments are usually reset to a random state, which help to test the robustness of control algorithms. You can make the random state reproducibly by passing a [seed](https://en.wikipedia.org/wiki/Random_seed) to a [reset function](https://gymnasium.farama.org/api/env/#gymnasium.Env.reset). (Seeding is a common concepts for random number generators - it is useful to read up on this if you haven't done so.) Some environments, such as [crazyflow, also allow to set the initial state explicitely](https://github.com/utiasDSL/crazyflow/blob/d87bc1eaf100e7d8927731c630e52a7163108ecf/crazyflow/gymnasium_envs/crazyflow.py#L150), which is useful during development of control algorithms.\n",
    "\n",
    ">**Note**: if you come from a control background, you already know about the **state** of a system. A state must not be confused with **observations**. Typically, the observation is a subspace of the state that the policy receives as input to calculate the actions. An observation can also be an RGB image obtained from a camera. Its important to understand the [difference between states and observations](https://spinningup.openai.com/en/latest/spinningup/rl_intro.html#states-and-observations)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 Drone simulation and environment\n",
    "Throughout the semester, you will develop controllers for a simulation of the [Crazyflie 2.1](https://www.bitcraze.io/products/old-products/crazyflie-2-1/). Here, we introduce the dynamics model, the simulation, and gymnasium environment.\n",
    "\n",
    "##### Drone dynamics model\n",
    "\n",
    " We use the following second-order time-continuous dynamics model of the drone. The state $x$ is the position of its center of mass $(x, y, z)$, the body orientations roll, pitch, and yaw $(\\phi, \\theta, \\psi)$, and their respective derivatives. The control inputs $u$ are the vertical thrust $F_z$ and the desired roll pitch yaw angles $(\\phi_d, \\theta_d, \\psi_d)$ around each axis of a coordinate frame fixed to the quadrotor body. This is a so-called [attitude control interface](https://www.bitcraze.io/documentation/repository/crazyflie-firmware/master/functional-areas/sensor-to-control/controllers/). $g$ is gravity.\n",
    "$$\n",
    "\\begin{equation} \\tag{1}\n",
    "    \\frac{d}{dt}\\begin{bmatrix} x\\\\ \\dot{x}\\\\ y\\\\ \\dot{y}\\\\ z\\\\ \\dot{z}\\\\ \\phi\\\\ \\theta\\\\ \\psi\\\\ \\dot{\\phi}\\\\ \\dot{\\theta}\\\\ \\dot{\\psi} \\end{bmatrix}\n",
    "    = \n",
    "    \\begin{bmatrix} \\dot{x}\\\\ 0\\\\ \\dot{y}\\\\  0\\\\ \\dot{z}\\\\ -g\\\\ \\dot{\\phi}\\\\ \\dot{\\theta}\\\\ \\dot{\\psi}\\\\ \\alpha_{\\phi}\\phi + \\gamma_{\\phi}\\dot{\\phi}\\\\ \\alpha_{\\theta}\\theta + \\gamma_{\\theta}\\dot{\\theta}\\\\ \\alpha_{\\psi}\\psi + \\gamma_{\\psi}\\dot{\\psi} \\end{bmatrix} \n",
    "    + \n",
    "    \\begin{bmatrix} \n",
    "    0 & 0 & 0 & 0\\\\\n",
    "    \\frac{1}{m} (\\cos\\phi \\sin\\theta \\cos\\psi + \\sin\\phi \\sin\\psi) & 0 & 0 & 0\\\\ \n",
    "    0 & 0 & 0 & 0\\\\\n",
    "    \\frac{1}{m} (\\cos\\phi \\sin\\theta \\sin\\psi - \\sin\\phi \\cos\\psi) & 0 & 0 & 0\\\\ \n",
    "    0 & 0 & 0 & 0\\\\\n",
    "    \\frac{1}{m} \\cos\\phi \\cos\\theta & 0 & 0 & 0\\\\\n",
    "    0 & 0 & 0 & 0\\\\\n",
    "    0 & 0 & 0 & 0\\\\\n",
    "    0 & 0 & 0 & 0\\\\\n",
    "    0 & \\beta_{\\phi} & 0 & 0\\\\\n",
    "    0 & 0 & \\beta_{\\theta} & 0\\\\\n",
    "    0 & 0 & 0 & \\beta_{\\psi}\n",
    "    \\end{bmatrix} u, \n",
    "\\end{equation}\n",
    "$$\n",
    "with\n",
    "$$ \n",
    "\\begin{equation} \\tag{2}\n",
    "    u = (F_z, \\phi_d, \\theta_d, \\psi_d).\n",
    "\\end{equation}\n",
    "$$\n",
    "\n",
    "The parameters $\\alpha, \\beta, \\gamma, m$ of the model have been determined using system identification. For this, we collected data and fitted the parameters using a least-square error.\n",
    "\n",
    "In a general form, such a system can be written as\n",
    "\n",
    "\n",
    "$$\n",
    "\\begin{equation} \\tag{3}\n",
    "    \\dot{x} = f(x) + g(x)\\cdot u.\n",
    "\\end{equation}\n",
    "$$\n",
    "\n",
    "\n",
    "\n",
    "For the simulation on a computer, we require a time-discretized version of the model for each timestep $k$ that approximates the next state:\n",
    "$$\n",
    "\\begin{align} \\tag{4}\n",
    "    \\frac{x_{x+1}-x_k}{\\delta t} &= f(x_k) + g(x_k) \\cdot u_k \\\\\n",
    "    \\tag{5}\n",
    "    x_{k+1} &= \\delta t (f(x_k) + g(x_k) \\cdot u_k) + x_k\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "Equation 5) can be used to step the dynamics model in time on a computer. There are different methods to solve this integration step. More advanced methods, such as RK4, approximate the gradient during $\\delta t$ more precisely.\n",
    "\n",
    "We implement this discretized version of the model in a simulation that we call [`crazyflow`](https://github.com/utiasDSL/crazyflow). This simulation is optimized for speed and can run highly-efficient parallalized on the GPU.\n",
    "\n",
    "Additionally, we provide the time-continuous model as a [CasADi](https://web.casadi.org/) function. CasADi is a general-purpose tool for gradient-based numerical optimization – with a strong focus on optimal control. It implement algorithmic differentiation (AD), but also tools for numerical integration. Internal CasADi also uses an integrator to discretize the model.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Simulation\n",
    "\n",
    "An efficient, highly-parallizable (GPU) simulation of the discretized dynamics models is implemented in our simulator [`crazyflow`](https://github.com/utiasDSL/crazyflow).\n",
    "\n",
    "- The simulation is implemented in [JAX](https://docs.jax.dev/en/latest/quickstart.html), which is basically numpy for the GPU, and enables us to massively parallize the simulation.\n",
    "- The simulation provides physics, integrators (Euler or RK4), lower level controllers, and visualisation routines. If you are interested, you can check out the [this main function](https://github.com/utiasDSL/crazyflow/blob/d87bc1eaf100e7d8927731c630e52a7163108ecf/crazyflow/sim/sim.py) of the simulation.\n",
    "- The default simulation step-size is $2\\text{ms}$. The simulation provides Explicit Euler or RK4 for integration. We recommend to stick with the values that have been set for this exercise.\n",
    "- For visualisation, we use [MuJoCo](https://mujoco.org/), or more specific [MJX](https://mujoco.readthedocs.io/en/stable/mjx.html). While MuJoCo also comes with its own physics engine, we only use it for vizualization in crazyflow.\n",
    "\n",
    "> **Note**: Crazyflow is of course already installed in the container. You can find the code in `/home/vscode/venv/lib/python3.11/site-packages/crazyflow`, but **do not make any changes**. For the exercises, we use crazyflow as of commit [`d87bc1e`](https://github.com/utiasDSL/crazyflow/tree/d87bc1eaf100e7d8927731c630e52a7163108ecf).\n",
    "\n",
    "\n",
    "\n",
    "##### Gymnasium environment\n",
    "\n",
    "The simulation is wrapped with different [Gymnasium environments](https://github.com/utiasDSL/crazyflow/blob/d87bc1eaf100e7d8927731c630e52a7163108ecf/crazyflow/gymnasium_envs/crazyflow.py) that define different tasks. You will use the environments to interact with the drone simulation using your controllers, as explained in the previous section. We provide [multiple environments](https://github.com/utiasDSL/crazyflow/blob/d87bc1eaf100e7d8927731c630e52a7163108ecf/crazyflow/gymnasium_envs/crazyflow.py), each for a different purpose. The environments differ in their goals (reward formulations), observation spaces, and other details. For instance, we have `CrazyflowEnvReachGoal` and `CrazyflowEnvFigureEightTrajectory`. We will use the different environments throughout the exercises.\n",
    "- **Observation space**: The observation spaces differ for the different gymnasium environments. However, each observation includes at least the state of the drone, which are `position`, `orientation` (in quaternion), and its derivatives. Some environments include additional observations, e.g. the `CrazyflowEnvReachGoal` additionally includes the `difference_to_goal`.\n",
    "- **Action space** (corresponds to control input $u$): We use the [Attitude controll interface](https://www.bitcraze.io/documentation/repository/crazyflie-firmware/master/functional-areas/sensor-to-control/controllers/) of the drone, as specified in the dynamics model above. This interface takes **four control inputs**: `[collective thrust, roll, pitch, yaw]`. Thus, the action space is a [continuous box](https://gymnasium.farama.org/api/spaces/fundamental/#gymnasium.spaces.Box) represented as <b>float32</b>. The range for the thrust is <b>0.11264675 to 0.5933658</b>, while the orientations range from <b>-π to π</b>. Actions needs to be a 2D array of shape `(n_envs, n_actions)`. For all exercises except the last one, we always have `n_envs=1` and `n_actions=4`.\n",
    "\n",
    "\n",
    "> **Note**: There are different ways to represent orientations in 3D space. The observations of the environment returns [quaternions](https://en.wikipedia.org/wiki/Quaternions_and_spatial_rotation), while the symbolic model works with Euler Angles. You do not have to worry too much about that, as you will use [this scipy package](https://docs.scipy.org/doc/scipy/reference/generated/scipy.spatial.transform.Rotation.html) to easily convert orientation representations.\n",
    "\n",
    "> **Note**: The actions need to be a 2D array of shape `(n_envs, n_actions)`. Except for parallel simulations on GPU for learning based methods, we set `n_envs=1`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create an environment of type `DroneReachPos-v0`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# init env\n",
    "import crazyflow  # register gymnasium envs\n",
    "import gymnasium as gym\n",
    "from gymnasium.wrappers.vector import JaxToNumpy\n",
    "\n",
    "SEED = 42  # Seeding is very important for reproducibility; https://x.com/jakevdp/status/1247742792861757441\n",
    "\n",
    "# Initialize the environment with various parameters.\n",
    "env = gym.make_vec(\n",
    "    \"DroneReachPos-v0\",\n",
    "    num_envs=1,  # we will always use num_envs=1\n",
    "    freq=500,\n",
    "    device=\"cpu\",  # running on CPU is sufficient\n",
    "    render_goal_marker=True,  # only for visualization\n",
    ")\n",
    "\n",
    "# Use a wrapper (https://gymnasium.farama.org/api/wrappers/) to convert the JAX array from the simulation to numpy arrays.\n",
    "env = JaxToNumpy(env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get obs and act spaces\n",
    "print(f\"Observation space for a drone: {env.observation_space}\")\n",
    "# NOTE Note the limits for the action space, and recall their meanings! You will need it in further exercises\n",
    "print(f\"Action space for a drone: {env.action_space}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the simulation using random control inputs.\n",
    "\n",
    "Hint: You can use the `Tab`-Key to switch between cameras."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A simple simulation loop\n",
    "\n",
    "# An examplary action for going up in attitude control\n",
    "# We can use numpy arrays as we applied the JaxToNumpy wrapper.\n",
    "# Recall that actions are of shape (1, 4)\n",
    "action = np.zeros((1, 4), dtype=np.float32)\n",
    "action[..., 0] = 0.4\n",
    "\n",
    "obs, info = env.reset(seed=SEED)\n",
    "\n",
    "# Step through the environment\n",
    "for _ in range(1000):\n",
    "    observation, reward, terminated, truncated, info = env.step(action)\n",
    "    env.render()  # comment this out for much faster simulation\n",
    "    env.unwrapped.sim.viewer.viewer.cam.lookat = env.unwrapped.goal[0]\n",
    "    if terminated or truncated:\n",
    "        env.reset()\n",
    "env.close()\n",
    "env.unwrapped.sim.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\">\n",
    "    <h3>Task 1: Exam Preparation</h3>\n",
    "    <p>\n",
    "    Why is the drone not only moving up in simulation but also in other directions, although the provided action vector points upwards?\n",
    "    </p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Symbolic model\n",
    "\n",
    "As explained above, we provide the drone dynamics also as a [CasADi](https://web.casadi.org/) symbolic model. CasADi is a general-purpose tool for gradient-based numerical optimization – with a strong focus on optimal control. It implement algorithmic differentiation (AD), but also tools for numerical integration. Internally, of course, CasADi uses also an integrator to discretize the model. You will need the symbolic model in the following exercises to implement model-based control algorithms, such as MPC.\n",
    "\n",
    "We implemented the convinience function `symbolic_from_sim` in [`crazyflow/sim/symbolic.py`](https://github.com/utiasDSL/crazyflow/blob/d87bc1eaf100e7d8927731c630e52a7163108ecf/crazyflow/sim/symbolic.py) to get a symbolic CasADi model with very similar behavior to the simulation. The continuous time dynamics model is stored in the [`fc_func`](https://github.com/utiasDSL/crazyflow/blob/bb429846791ee400dc18bddc3bed904ae4e6cb04/crazyflow/sim/symbolic.py#L72) property, and the discrete time dynamics in [`fd_func`](https://github.com/utiasDSL/crazyflow/blob/bb429846791ee400dc18bddc3bed904ae4e6cb04/crazyflow/sim/symbolic.py#L74). In the following, we demonstrate that the simulation and symbolic model indeed behave very similar. Slight differences arise due to numerical effects and different integrators used by our simulation and CasADi."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\">\n",
    "    <h3>Task 2: Check code.</h3>\n",
    "    <p>\n",
    "      To prepare for the next exercise, you can read the <code>__init__</code>, <code>setup_model</code> and <code>setup_linearization</code> methodes of the <a href=\"https://github.com/utiasDSL/crazyflow/blob/d87bc1eaf100e7d8927731c630e52a7163108ecf/crazyflow/sim/symbolic.py#L27\"><code>SymbolicModel</code></a> class in <a href=\"https://github.com/utiasDSL/crazyflow/blob/d87bc1eaf100e7d8927731c630e52a7163108ecf/crazyflow/sim/symbolic.py\"><code>crazyflow/sim/symbolic.py</code></a>. Understand how the symbolic representations of the drone’s dynamics, state, and cost functions are created. To understand this, you need some familarity with <a href=\"https://web.casadi.org/\">CasADi</a> - now would be a good time to read up on this.\n",
    "    </p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\">\n",
    "    <h3>Task 3: Check code.</h3>\n",
    "    <p>\n",
    "      Check the implementation in the next code cell. You can play around with the identified parameters of the simulation model, and observe how the simulation and symbolic model diverge. You can do so by changing the parameters in the codeline after `TODO`.\n",
    "    </p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from crazyflow.control.control import MAX_THRUST, MIN_THRUST\n",
    "from crazyflow.sim.symbolic import symbolic_from_sim\n",
    "\n",
    "# create env\n",
    "envs = gym.make_vec(\"DroneReachPos-v0\", num_envs=1, freq=500, device=\"cpu\")\n",
    "# obtain symbolic model\n",
    "symbolic_model = symbolic_from_sim(envs.sim)\n",
    "\n",
    "# TODO: change the parameters to see how the simulation and symbolic model diverge\n",
    "crazyflow.sim.physics.SYS_ID_PARAMS[\"acc\"] = np.array([20.907574256269616, 3.653687545690674])\n",
    "\n",
    "# Prepare variables for logging\n",
    "state_names = [\"x\", \"x_dot\", \"y\", \"y_dot\", \"z\", \"z_dot\"]\n",
    "sim_log = {name: [] for name in state_names}\n",
    "sym_log = {name: [] for name in state_names}\n",
    "\n",
    "obs, info = envs.reset(seed=42)\n",
    "\n",
    "pos = obs[\"pos\"].squeeze()  # shape: (3,)\n",
    "vel = obs[\"vel\"].squeeze()  # shape: (3,)\n",
    "state = np.array([pos[0], vel[0], pos[1], vel[1], pos[2], vel[2]])\n",
    "\n",
    "init_state = np.array(\n",
    "    [pos[0], vel[0], pos[1], vel[1], pos[2], vel[2], 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
    ")  # only randomize pos and vel\n",
    "\n",
    "for i, name in enumerate(state_names):\n",
    "    sim_log[name].append(init_state[i])\n",
    "    sym_log[name].append(init_state[i])\n",
    "\n",
    "xf = init_state\n",
    "num_steps = 200\n",
    "for i in range(num_steps):\n",
    "    # Generate random input\n",
    "    np.random.seed(seed=i)\n",
    "    random_input = np.array(\n",
    "        [\n",
    "            np.random.uniform(4 * MIN_THRUST, 4 * MAX_THRUST),  # thrust\n",
    "            np.random.uniform(-np.pi, np.pi),  # roll\n",
    "            np.random.uniform(-np.pi, np.pi),  # pitch\n",
    "            np.random.uniform(-np.pi, np.pi),  # yaw\n",
    "        ],\n",
    "        dtype=np.float32,\n",
    "    ).reshape((1, 4))\n",
    "\n",
    "    # Step (discretized) symbolic model\n",
    "    res = symbolic_model.fd_func(x0=xf, p=random_input)\n",
    "    xf = res[\"xf\"].full().flatten()\n",
    "\n",
    "    for j, name in enumerate(state_names):\n",
    "        sym_log[name].append(xf[j])\n",
    "\n",
    "    # Step simulation\n",
    "    obs, reward, terminated, truncated, info = envs.step(random_input)\n",
    "    pos = obs[\"pos\"].squeeze()  # shape: (3,)\n",
    "    vel = obs[\"vel\"].squeeze()  # shape: (3,)\n",
    "    state = np.array([pos[0], vel[0], pos[1], vel[1], pos[2], vel[2]])\n",
    "\n",
    "    for j, name in enumerate(state_names):\n",
    "        sim_log[name].append(state[j])\n",
    "\n",
    "envs.close()\n",
    "envs.unwrapped.sim.close()\n",
    "\n",
    "# Plot results\n",
    "time = np.arange(len(sym_log[\"x\"]))\n",
    "fig, axes = plt.subplots(2, 3, figsize=(16, 12))\n",
    "fig.suptitle(\"Symbolic vs Simulation Model Comparison\")\n",
    "variables = [(\"x\", \"y\", \"z\"), (\"x_dot\", \"y_dot\", \"z_dot\")]\n",
    "colors = [\"blue\", \"orange\", \"green\"]\n",
    "\n",
    "\n",
    "def plot_variable(ax, var_name, color=\"blue\"):\n",
    "    ax.plot(time, sym_log[var_name], label=f\"{var_name}_symbolic\", color=color)\n",
    "    ax.plot(time, sim_log[var_name], label=f\"{var_name}_sim\", color=color, linestyle=\"--\")\n",
    "    ax.set_title(f\"{var_name} trajectory\")\n",
    "    ax.set_xlabel(\"Time Steps\")\n",
    "    ax.set_ylabel(var_name)\n",
    "    ax.legend()\n",
    "    ax.grid(True)\n",
    "\n",
    "\n",
    "for row, var_row in enumerate(variables):\n",
    "    for col, var in enumerate(var_row):\n",
    "        plot_variable(axes[row, col], var, colors[col])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4 PD Controller\n",
    "\n",
    "Next, we implement a simple PD controller that enables the drone to fly to a target location and hover there. The PD controller takes as input a desired position (and velocity and yaw), as well as the controller gains `kp, kd`, and the current state of the drone. Note that the gymnasium environment returns the angular observations as [quaternions](https://en.wikipedia.org/wiki/Quaternions_and_spatial_rotation). The controller returns the attitude command consisting of collective thrust, roll, pitch, and yaw, as explained above.\n",
    "\n",
    "Hint: you can use the `Tab` key to switch between cameras."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\">\n",
    "    <h3>Task 4: Check code.</h3>\n",
    "    <p>\n",
    "      Go to the implementation of the PD class in <code>exercise01/PDDrone.py</code>. It inherits from a <code>BaseController</code> class that we will use throughout the following exercises.\n",
    "    </p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\">\n",
    "    <h3>Task 5: Exam preparation.</h3>\n",
    "    <p>\n",
    "    Play around with the <code>kp</code> and <code>kd</code> gains in the below code cell. In particular, observe what happens for high control gains? Do you think the action space limitations are chosen appropriately? You can also adjust the desired position, or the initial state (using the random seed or by providing custom initial conditions). Use the below code cells to generate plots.\n",
    "    </p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\">\n",
    "    <h3>Task 6: Exam preparation.</h3>\n",
    "    <p>\n",
    "    Change the drone mass used in the controller in the below code cell and observe the effect. How can the controller be improved to compensate for errors in the drone mass estimation?\n",
    "    </p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from crazyflow.constants import GRAVITY, MASS\n",
    "from PD import PDDrone\n",
    "\n",
    "# Logging\n",
    "action_log = []\n",
    "\n",
    "# TODO play around with different desired positions\n",
    "desired_position = np.array([0.0, 0.0, 1.5])\n",
    "\n",
    "# TODO play around with different controller gains. Specifically, try high gains and observe the effects of running into actuation limits.\n",
    "# TODO change the drone mass used in the controller and observe the effect. How can the controller be improved to compensate for errors in the drone mass estimation?\n",
    "controller = PDDrone(\n",
    "    des_pos=desired_position,\n",
    "    kp=np.array([0.4, 0.4, 1.25]),  # kp gain\n",
    "    kd=np.array([0.2, 0.2, 0.5]),  # kd gain\n",
    "    drone_mass=MASS,\n",
    ")\n",
    "# TODO play around with different initial conditions. You can also set specific initial conditions by passing the \"options\" dict to the env.reset() method: https://github.com/utiasDSL/crazyflow/blob/d87bc1eaf100e7d8927731c630e52a7163108ecf/crazyflow/gymnasium_envs/crazyflow.py#L162\n",
    "observation, _ = env.reset(seed=SEED)\n",
    "\n",
    "\n",
    "env.unwrapped.goal = env.unwrapped.goal.at[...].set(\n",
    "    desired_position\n",
    ")  # required for visualization of the goal marker\n",
    "\n",
    "for i in range(1000):\n",
    "    pos, vel, quat = observation[\"pos\"], observation[\"vel\"], observation[\"quat\"]\n",
    "    action = controller.step_control(pos, vel, quat)\n",
    "    action = np.clip(\n",
    "        action, env.unwrapped.action_space.low, env.unwrapped.action_space.high\n",
    "    )  # bound actions otherwise they are rejected by the step function\n",
    "    observation, _, terminated, truncated, _ = env.step(action)\n",
    "\n",
    "    # Logging\n",
    "    action_log.append(action)\n",
    "\n",
    "    # skip frames for faster rendering\n",
    "    if i % 5 == 0:\n",
    "        env.render()\n",
    "        # focus camera on target\n",
    "        env.unwrapped.sim.viewer.viewer.cam.lookat = env.unwrapped.goal[0]\n",
    "    if terminated or truncated:\n",
    "        env.reset()\n",
    "env.close()\n",
    "env.unwrapped.sim.close()\n",
    "\n",
    "action_log = np.array(action_log)\n",
    "\n",
    "print(f\"Final position of the drone: {observation['pos']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Plot results\n",
    "steps = np.arange(len(action_log))\n",
    "fig, axes = plt.subplots(1, 4, figsize=(20, 4))  # 1 row, 4 columns\n",
    "fig.suptitle(\"Action logs\")\n",
    "variables = [\"thrust\", \"roll\", \"pitch\", \"yaw\"]\n",
    "\n",
    "\n",
    "def plot_variable(ax, i):\n",
    "    ax.plot(steps, action_log[:, 0, i], label=variables[i])\n",
    "    ax.axhline(env.unwrapped.action_space.low[0][i], color=\"red\", linestyle=\"--\", label=\"min\")\n",
    "    ax.axhline(env.unwrapped.action_space.high[0][i], color=\"green\", linestyle=\"--\", label=\"max\")\n",
    "    if i == 0:\n",
    "        ax.axhline(\n",
    "            1.8 * controller.drone_mass * GRAVITY,\n",
    "            color=\"black\",\n",
    "            linestyle=\"--\",\n",
    "            label=\"max (controller)\",\n",
    "        )\n",
    "        ax.axhline(\n",
    "            0.3 * controller.drone_mass * GRAVITY,\n",
    "            color=\"grey\",\n",
    "            linestyle=\"--\",\n",
    "            label=\"min (controller)\",\n",
    "        )\n",
    "    ax.set_xlabel(\"Time Steps\")\n",
    "    ax.set_ylabel(variables[i])\n",
    "    ax.legend()\n",
    "    ax.grid(True)\n",
    "\n",
    "\n",
    "for i in range(4):\n",
    "    plot_variable(axes[i], i)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">**Note**: for the plots you will observe maximal thrusts that get clipped before reaching the maximum thurst of 0.59 defined by the drone environment. The reason for that is that the thrust value gets clipped to even lower values in the PD controller implementation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great! Now you understand how to use the drone environment 🚀! In the coming exercises you will develop some cool controllers for it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5 Example submission\n",
    "\n",
    "Lastly for this introduction, we want you to get familiar with the python test cases and the submission system. **If you have not done so already, please carefully read the information in the `README.md` file regarding test cases and submissions**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\">\n",
    "    <h3>Task 7: Implement return_true()</h3>\n",
    "    <p>\n",
    "      Please implement the function <code>return_true()</code> in <code>exercise01/return_true.py</code>. Run the corresponding tests for <img src=\"../resources/test_icon.png\" alt=\"test_icon\" width=\"20\"/> <code>test/behavior/test_exercise01</code> locally on your computer to see if you implemented the function correctly. Try some faulty implementations as well, and observe the feedback you get. \n",
    "    </p>\n",
    "    <p>\n",
    "     When you pass all tests locally on your computer go ahead and submit to ARTEMIS. The tests should complete successfully on ARTEMIS as well.\n",
    "    </p>\n",
    "    <p>\n",
    "    This task does not count towards your final grade (and thus doesn't include any hidden test cases).\n",
    "    </p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Execute the following cell as well to see the output of your implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "from return_true import return_true\n",
    "\n",
    "return_value = return_true()\n",
    "\n",
    "print(f\"return_value is{'' if isinstance(return_value, bool) else ' not'} a boolean.\")\n",
    "print(f\"return_value is: {return_value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### That's it!\n",
    "You are done for this exercise. See you in the next one!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
