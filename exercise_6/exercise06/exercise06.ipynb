{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 6: Deep Reinforcement Learning (DRL)\n",
    "\n",
    "In DRL, we do not require a symbolic model of our system, only a (highly-parallizable) simulation. The policy is then learned directly from the data collected of the simulated robot interacting with its environment. As much data is needed for training (typically millions of samples), the efficient simulation implemented by [crazyflow](https://github.com/utiasDSL/crazyflow) will come in handy.\n",
    "\n",
    "In the lecture you, you were introduced to the **on-policy DRL PPO algorithm**, as it has become the most popular for robotics applications. In this exercise, you will use **PPO** to learn to fly a flying a figure eight trajectory. With a drone."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\">\n",
    "    <h3>Task 1: WandB</h3>\n",
    "    <p>\n",
    "    Go to <a href=\"https://wandb.ai/site\" target=\"_blank\">https://wandb.ai/site</a> and create a free WandB account. WandB is a very common tool in tracking deep learning experiments, and we will use it to track our training in this exercise. WandB includes 100GB of free online storage, whis is more than enough for our use case. Once you have created your account, execute the following code cell. Follow the instructions to obtain your API key and paste it as requested. If everything was successful, you should see a massage similar to <code>\"wandb: Currently logged in as: ...</code>\n",
    "    </p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter:\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: No netrc file found, creating one.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /home/vscode/.netrc\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33moliefr\u001b[0m to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "import torch\n",
    "import wandb\n",
    "from ml_collections import ConfigDict\n",
    "from ppo import PPOTrainer, set_seeds\n",
    "\n",
    "wandb.login()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\">\n",
    "    <h3>Task 2 (Optional): Setup GPU</h3>\n",
    "    <p>\n",
    "    This exercise benefits from running the training on a powerful <a href=\"https://developer.nvidia.com/cuda-gpus\" target=\"_blank\">cuda-enabled GPU</a>. If you haven't done so yet, you can easily setup your container to run on the GPU. Consult the <code>README.md</code> for the instruction.\n",
    "    </p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "train_device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using device: {train_device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note on  (CPU / GPU)**: There are two bottlenecks that determine the speed of any DRL training: a) The simulation (data collection) and b) the learning of the agent (optimization). Both can have there on device to run on, i.e., CPU or GPU. Unfortunately, choosing a device for a simulation is not always straightforward, as the simulation can be faster on either device, depending on the specific simulation and parallelization. For reference see [the example in MJX here](https://mujoco.readthedocs.io/en/stable/mjx.html#mjx-the-sharp-bits). If the simulation is run on CPU, it can make sense to run the optimization on the CPU as well, as moving tensors from CPU to GPU takes time.\n",
    "\n",
    "For crazyflow, we observe that simulation on GPU is faster if you run more than ~32 environments in parallel (depending on the specific CPU and GPU). A typical number of parallel environments to use for DRL is for instance 4096. At this point, the GPU provides significant speed ups as crazyflow is implemented in a way that the tensors stay on the GPU throughout the entire training process.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:green\">As we will execute the entirety of the training code using a central config object defined below, all implementations are contained in external `.py` files. Make use of the local testing feature to test your implementations for the below tasks to make sure the training runs smoothly in the end!</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\">\n",
    "    <h3>Task 3: Deterministic Seeding for Reproducible PPO Training</h3>\n",
    "    <p>\n",
    "      In the provided <code>ppo.py</code>, implement the <code>set_seeds</code> function so that it ensures complete reproducibility across all sources of randomness.\n",
    "    </p>\n",
    "</div>\n",
    "\n",
    "Specifically, your function must:\n",
    "\n",
    "  - Set the Python built-in random module’s seed.\n",
    "\n",
    "  - Seed NumPy’s random number generator.\n",
    "\n",
    "  - Seed PyTorch’s CPU and all GPU (CUDA) random generators.\n",
    "\n",
    "  - Configure torch.backends.cudnn for fully deterministic behavior by enabling `deterministic = True` and disabling `benchmark = False`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\">\n",
    "    <h3>Task 4: Implement make_envs</h3>\n",
    "    <p>\n",
    "      Implement the <code>make_envs</code> function in <code>ppo.py</code> to construct two parallel Gymnasium vector environments.\n",
    "    </p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\">\n",
    "    <h3>Task 5: Exam Preparation: Normalization</h3>\n",
    "    <p>\n",
    "        Why do we need to normalize observations? (The reason is the same as for Neural Networks.)\n",
    "    </p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `save_model` function packages everything needed to resume training or run inference—namely the agent’s network weights, optimizer state, and the environment’s normalization statistics—into a single checkpoint file. This ensures you can stop and later restart training seamlessly, and that input normalization remains consistent at test time.\n",
    "\n",
    "<div class=\"alert alert-info\">\n",
    "    <h3>Task 6: Implement save_model</h3>\n",
    "    <p>\n",
    "      Implement the <code>save_model</code> function in <code>ppo.py</code>.\n",
    "    </p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\">\n",
    "    <h3>Task 7: Synchronize Observation Normalization</h3>\n",
    "    <p>\n",
    "      Implement the <code>sync_envs(train_envs, eval_envs)</code> function to ensure that the evaluation environment uses the exact same observation normalization statistics as the training environment.\n",
    "    </p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\">\n",
    "    <h3>Task 8: Check Code and Exam Preparation</h3>\n",
    "   <p>Have a look at the implementation of <code>Agent</code> class in <code>agent.py</code>. Answer the following questions:\n",
    "   \n",
    "   - Network initialization: Why do you have to be careful with the way you initialize the policy network? Have a look at <a href=\"https://arxiv.org/pdf/2006.05990\">https://arxiv.org/pdf/2006.05990</a> at page 5 to answer the question.\n",
    "   - In the <code>ppo.py</code> file, in <code>PPOTrainer.learn</code>, you can see an `entropy_loss` term in the computation of the overall loss (`loss = ...`). This entropy is computed in the `action_and_value` function in `agent.py` from the action probabilities. What would you expect to happen during training if you increase the weight of the entropy factor in the loss function (`self.config.ent_coef`)? For this, it is important that you know what the entropy of a distribution represents. You can look it up <a href=\"https://en.wikipedia.org/wiki/Entropy_(information_theory)\">on wikipedia</a>. <a href=\"https://iclr-blog-track.github.io/2022/03/25/ppo-implementation-details/\">This article</a> under point 10. also helps!\n",
    "</p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\">\n",
    "    <h3>Task 9: Generalized Advantage Estimate</h3>\n",
    "    <p>\n",
    "      You already learned about the PPO loss in the lecture. The PPO loss calculation requires the calculation of so-called Advantages. For an explanation, see the Script in Eq. 8.39, 8.40, and 8.42. Calculating the advantages is a critical design decision in DRL algorithms as it heavily influences the learning. A standard method is the Generalized Advantage Estimate (GAE).\n",
    "<p></p>\n",
    "      GAE is a bit more involved and we already implemented a part of it. However, you task is to complete the function. Head over to <code>ppo.py</code> and implement the <code>calculate_advantages</code> method of the <code>PPOTrainer</code> class. You will need to refer to one equation in the original GAE paper: <a href=\"https://arxiv.org/pdf/1506.02438\">https://arxiv.org/pdf/1506.02438</a>.\n",
    "    </p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\">\n",
    "    <h3>Task 10: PPO-Clip Policy Gradient Loss</h3>\n",
    "    <p>\n",
    "      Go to the <code>ppo.py</code>, implement the <code>calculate_pg_loss</code> method of <code>PPOTrainer</code> class to compute the policy gradient loss.\n",
    "    </p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PPO Value Function Loss\n",
    "\n",
    "####  Case 1: **Unclipped Value Loss**\n",
    "\n",
    "When `if_clip = False`, the value loss is computed using the standard Mean Squared Error (MSE) between predicted values and target returns:\n",
    "\n",
    "$$\n",
    "\\mathcal{L}_v = \\frac{1}{2} \\cdot \\mathbb{E}_t \\left[ \\left( V_{\\theta}(s_t) - \\hat{R}_t \\right)^2 \\right]\n",
    "$$\n",
    "\n",
    "- $ V_{\\theta}(s_t) $: Current value function prediction (`newvalue`)\n",
    "- $ \\hat{R}_t $: Target return at time step $ t $ (from GAE, i.e., `b_returns`)\n",
    "- The factor $ \\frac{1}{2} $ is conventional in MSE to simplify derivative expressions.\n",
    "\n",
    "---\n",
    "\n",
    "#### Case 2: **Clipped Value Loss**\n",
    "\n",
    "When `if_clip = True`, PPO applies clipping to the value function update to prevent large deviations from the old value estimate.\n",
    "\n",
    "\n",
    "##### **Step 1: Clipped Value Prediction**\n",
    "\n",
    "$$\n",
    "v_t^{\\text{clip}} = V_{\\theta_{\\text{old}}}(s_t) + \\text{clip}\\left(V_{\\theta}(s_t) - V_{\\theta_{\\text{old}}}(s_t),\\; -\\epsilon,\\; +\\epsilon \\right)\n",
    "$$\n",
    "\n",
    "##### **Step 2: Loss Calculation**\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\ell_{\\text{unclip}} &= \\left(V_{\\theta}(s_t) - \\hat{R}_t\\right)^2 \\\\\n",
    "\\ell_{\\text{clip}} &= \\left(v_t^{\\text{clip}} - \\hat{R}_t\\right)^2 \\\\\n",
    "\\mathcal{L}_v &= \\frac{1}{2} \\cdot \\mathbb{E}_t \\left[ \\max\\left( \\ell_{\\text{unclip}},\\; \\ell_{\\text{clip}} \\right) \\right]\n",
    "\\end{aligned}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\">\n",
    "    <h3>Task 11: Value Loss</h3>\n",
    "   <p> Go to the <code>ppo.py</code>, complete the implementation of <code>calculate_v_loss</code> method of <code>PPOTrainer</code> class.\n",
    "</p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\">\n",
    "    <h3>Task 12: Check code</h3>\n",
    "   <p> The <code>learn</code> method is the core part of the <code>PPO</code> algorithm — it is responsible for training both the policy and value networks. The method performs multiple optimization steps using previously collected experience samples (such as <code>observations, actions, log probabilities, returns, etc.</code>).</p>\n",
    "\n",
    "   <p>Have a look at the <code>loss=...`</code> line in the <code>`learn()`</code> function, where all loss components are accumulated.</p>\n",
    "\n",
    "After completing the previous tasks, you are encouraged to take a closer look at the overall workflow inside the <code>learn</code> method.\n",
    "</p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\">\n",
    "    <h3>Task 13: Train your Agent</h3>\n",
    "    <p>\n",
    "        Once you’ve finished the above tasks, run the following cell to train your PPO agent. Don't forget to commit and push your policy <code>(ppo_checkpoint_ex06.pt)</code> to ARTEMIS in the end!\n",
    "    </p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.7"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/workspaces/Advanced-Robot-Learning-and-Decision-Making-Programming-Exercises/src/exercise06/exercise06/wandb/run-20250808_144008-a4qatq71</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/oliefr/ARLDM-Exercise-PPO/runs/a4qatq71' target=\"_blank\">lyric-sponge-42</a></strong> to <a href='https://wandb.ai/oliefr/ARLDM-Exercise-PPO' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/oliefr/ARLDM-Exercise-PPO' target=\"_blank\">https://wandb.ai/oliefr/ARLDM-Exercise-PPO</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/oliefr/ARLDM-Exercise-PPO/runs/a4qatq71' target=\"_blank\">https://wandb.ai/oliefr/ARLDM-Exercise-PPO/runs/a4qatq71</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/vscode/venv/lib/python3.11/site-packages/gymnasium/envs/registration.py:982: UserWarning: \u001b[33mWARN: The VectorEnv (CrazyflowEnvFigureEightTrajectory(DroneFigureEightTrajectory-v0, num_envs=1024)) is missing AutoresetMode metadata, metadata={}\u001b[0m\n",
      "  warn(\n",
      "/home/vscode/venv/lib/python3.11/site-packages/gymnasium/envs/registration.py:982: UserWarning: \u001b[33mWARN: The VectorEnv (CrazyflowEnvFigureEightTrajectory(DroneFigureEightTrajectory-v0, num_envs=64)) is missing AutoresetMode metadata, metadata={}\u001b[0m\n",
      "  warn(\n",
      "/home/vscode/venv/lib/python3.11/site-packages/gymnasium/vector/vector_env.py:538: UserWarning: \u001b[33mWARN: Vector environment (<CrazyflowRL, CrazyflowEnvFigureEightTrajectory(DroneFigureEightTrajectory-v0, num_envs=1024)>) is missing `autoreset_mode` metadata key.\u001b[0m\n",
      "  warn(\n",
      "/home/vscode/venv/lib/python3.11/site-packages/gymnasium/vector/vector_env.py:538: UserWarning: \u001b[33mWARN: Vector environment (<FlattenJaxObservation, <CrazyflowRL, CrazyflowEnvFigureEightTrajectory(DroneFigureEightTrajectory-v0, num_envs=1024)>>) is missing `autoreset_mode` metadata key.\u001b[0m\n",
      "  warn(\n",
      "/home/vscode/venv/lib/python3.11/site-packages/gymnasium/wrappers/vector/stateful_observation.py:76: UserWarning: \u001b[33mWARN: <NormalizeObservation, <FlattenJaxObservation, <CrazyflowRL, CrazyflowEnvFigureEightTrajectory(DroneFigureEightTrajectory-v0, num_envs=1024)>>> is missing `autoreset_mode` data. Assuming that the vector environment it follows the `NextStep` autoreset api or autoreset is disabled. Read todo for more details.\u001b[0m\n",
      "  warn(\n",
      "/home/vscode/venv/lib/python3.11/site-packages/gymnasium/vector/vector_env.py:538: UserWarning: \u001b[33mWARN: Vector environment (<CrazyflowRL, CrazyflowEnvFigureEightTrajectory(DroneFigureEightTrajectory-v0, num_envs=64)>) is missing `autoreset_mode` metadata key.\u001b[0m\n",
      "  warn(\n",
      "/home/vscode/venv/lib/python3.11/site-packages/gymnasium/vector/vector_env.py:538: UserWarning: \u001b[33mWARN: Vector environment (<FlattenJaxObservation, <CrazyflowRL, CrazyflowEnvFigureEightTrajectory(DroneFigureEightTrajectory-v0, num_envs=64)>>) is missing `autoreset_mode` metadata key.\u001b[0m\n",
      "  warn(\n",
      "/home/vscode/venv/lib/python3.11/site-packages/gymnasium/wrappers/vector/stateful_observation.py:76: UserWarning: \u001b[33mWARN: <NormalizeObservation, <FlattenJaxObservation, <CrazyflowRL, CrazyflowEnvFigureEightTrajectory(DroneFigureEightTrajectory-v0, num_envs=64)>>> is missing `autoreset_mode` data. Assuming that the vector environment it follows the `NextStep` autoreset api or autoreset is disabled. Read todo for more details.\u001b[0m\n",
      "  warn(\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 31\u001b[0m\n\u001b[1;32m     29\u001b[0m set_seeds(train_config\u001b[38;5;241m.\u001b[39mseed)\n\u001b[1;32m     30\u001b[0m trainer \u001b[38;5;241m=\u001b[39m PPOTrainer(train_config, wandb_log\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m---> 31\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/workspaces/Advanced-Robot-Learning-and-Decision-Making-Programming-Exercises/src/exercise06/exercise06/ppo.py:655\u001b[0m, in \u001b[0;36mPPOTrainer.train\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    653\u001b[0m _ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrain_envs\u001b[38;5;241m.\u001b[39maction_space\u001b[38;5;241m.\u001b[39mseed(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mseed)\n\u001b[1;32m    654\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1000\u001b[39m):\n\u001b[0;32m--> 655\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_envs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtensor\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_envs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43maction_space\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msample\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    657\u001b[0m \u001b[38;5;66;03m# Main training loop\u001b[39;00m\n\u001b[1;32m    658\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m iteration \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mn_iterations \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m):\n\u001b[1;32m    659\u001b[0m     \u001b[38;5;66;03m# Decay learning rate\u001b[39;00m\n",
      "File \u001b[0;32m~/venv/lib/python3.11/site-packages/gymnasium/wrappers/vector/jax_to_torch.py:50\u001b[0m, in \u001b[0;36mJaxToTorch.step\u001b[0;34m(self, actions)\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Performs the given action within the environment.\u001b[39;00m\n\u001b[1;32m     42\u001b[0m \n\u001b[1;32m     43\u001b[0m \u001b[38;5;124;03mArgs:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[38;5;124;03m    Torch-based Tensors of the next observation, reward, termination, truncation, and extra info\u001b[39;00m\n\u001b[1;32m     48\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     49\u001b[0m jax_action \u001b[38;5;241m=\u001b[39m torch_to_jax(actions)\n\u001b[0;32m---> 50\u001b[0m obs, reward, terminated, truncated, info \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43mjax_action\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m (\n\u001b[1;32m     53\u001b[0m     jax_to_torch(obs, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice),\n\u001b[1;32m     54\u001b[0m     jax_to_torch(reward, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     57\u001b[0m     jax_to_torch(info, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice),\n\u001b[1;32m     58\u001b[0m )\n",
      "File \u001b[0;32m~/venv/lib/python3.11/site-packages/gymnasium/vector/vector_env.py:561\u001b[0m, in \u001b[0;36mVectorObservationWrapper.step\u001b[0;34m(self, actions)\u001b[0m\n\u001b[1;32m    557\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mstep\u001b[39m(\n\u001b[1;32m    558\u001b[0m     \u001b[38;5;28mself\u001b[39m, actions: ActType\n\u001b[1;32m    559\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mtuple\u001b[39m[ObsType, ArrayType, ArrayType, ArrayType, \u001b[38;5;28mdict\u001b[39m[\u001b[38;5;28mstr\u001b[39m, Any]]:\n\u001b[1;32m    560\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Modifies the observation returned from the environment ``step`` using the :meth:`observation`.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 561\u001b[0m     observations, rewards, terminations, truncations, infos \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43mactions\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m (\n\u001b[1;32m    563\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobservations(observations),\n\u001b[1;32m    564\u001b[0m         rewards,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    567\u001b[0m         infos,\n\u001b[1;32m    568\u001b[0m     )\n",
      "File \u001b[0;32m~/venv/lib/python3.11/site-packages/gymnasium/vector/vector_env.py:561\u001b[0m, in \u001b[0;36mVectorObservationWrapper.step\u001b[0;34m(self, actions)\u001b[0m\n\u001b[1;32m    557\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mstep\u001b[39m(\n\u001b[1;32m    558\u001b[0m     \u001b[38;5;28mself\u001b[39m, actions: ActType\n\u001b[1;32m    559\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mtuple\u001b[39m[ObsType, ArrayType, ArrayType, ArrayType, \u001b[38;5;28mdict\u001b[39m[\u001b[38;5;28mstr\u001b[39m, Any]]:\n\u001b[1;32m    560\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Modifies the observation returned from the environment ``step`` using the :meth:`observation`.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 561\u001b[0m     observations, rewards, terminations, truncations, infos \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43mactions\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m (\n\u001b[1;32m    563\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobservations(observations),\n\u001b[1;32m    564\u001b[0m         rewards,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    567\u001b[0m         infos,\n\u001b[1;32m    568\u001b[0m     )\n",
      "File \u001b[0;32m~/venv/lib/python3.11/site-packages/crazyflow/gymnasium_envs/crazyflow.py:618\u001b[0m, in \u001b[0;36mCrazyflowRL.step\u001b[0;34m(self, actions)\u001b[0m\n\u001b[1;32m    616\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mstep\u001b[39m(\u001b[38;5;28mself\u001b[39m, actions: Array) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mtuple\u001b[39m[\u001b[38;5;28mdict\u001b[39m, Array, Array, Array, \u001b[38;5;28mdict\u001b[39m]:\n\u001b[1;32m    617\u001b[0m     actions \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mclip(actions, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1.0\u001b[39m, \u001b[38;5;241m1.0\u001b[39m)\n\u001b[0;32m--> 618\u001b[0m     obs, reward, terminated, truncated, info \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mactions\u001b[49m\u001b[43m(\u001b[49m\u001b[43mactions\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    619\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m obs, reward, terminated, truncated, info\n",
      "File \u001b[0;32m~/venv/lib/python3.11/site-packages/crazyflow/gymnasium_envs/crazyflow.py:118\u001b[0m, in \u001b[0;36mCrazyflowBaseEnv.step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m    116\u001b[0m \u001b[38;5;66;03m# Reset all environments which terminated or were truncated in the last step\u001b[39;00m\n\u001b[1;32m    117\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m jnp\u001b[38;5;241m.\u001b[39many(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprev_done):\n\u001b[0;32m--> 118\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreset_masked\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprev_done\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    120\u001b[0m terminated \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mterminated\n\u001b[1;32m    121\u001b[0m truncated \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtruncated\n",
      "File \u001b[0;32m~/venv/lib/python3.11/site-packages/crazyflow/gymnasium_envs/crazyflow.py:484\u001b[0m, in \u001b[0;36mCrazyflowEnvFigureEightTrajectory.reset_masked\u001b[0;34m(self, mask)\u001b[0m\n\u001b[1;32m    477\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mreset_masked\u001b[39m(\u001b[38;5;28mself\u001b[39m, mask: Array) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    478\u001b[0m     reset_params \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m    479\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpos_min\u001b[39m\u001b[38;5;124m\"\u001b[39m: jnp\u001b[38;5;241m.\u001b[39marray([\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m0.1\u001b[39m, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m0.1\u001b[39m, \u001b[38;5;241m1.1\u001b[39m]),  \u001b[38;5;66;03m# x,y,z\u001b[39;00m\n\u001b[1;32m    480\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpos_max\u001b[39m\u001b[38;5;124m\"\u001b[39m: jnp\u001b[38;5;241m.\u001b[39marray([\u001b[38;5;241m0.1\u001b[39m, \u001b[38;5;241m0.1\u001b[39m, \u001b[38;5;241m1.3\u001b[39m]),  \u001b[38;5;66;03m# x,y,z\u001b[39;00m\n\u001b[1;32m    481\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvel_min\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m0.5\u001b[39m,\n\u001b[1;32m    482\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvel_max\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;241m0.5\u001b[39m,\n\u001b[1;32m    483\u001b[0m     }\n\u001b[0;32m--> 484\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreset_masked\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreset_params\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/venv/lib/python3.11/site-packages/crazyflow/gymnasium_envs/crazyflow.py:177\u001b[0m, in \u001b[0;36mCrazyflowBaseEnv.reset_masked\u001b[0;34m(self, mask, reset_params)\u001b[0m\n\u001b[1;32m    174\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInvalid bounds keys: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00minvalid_keys\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    175\u001b[0m     default_reset_params\u001b[38;5;241m.\u001b[39mupdate(reset_params)\n\u001b[0;32m--> 177\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msim\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreset\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmask\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    178\u001b[0m mask3d \u001b[38;5;241m=\u001b[39m mask[:, \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;28;01mNone\u001b[39;00m]\n\u001b[1;32m    179\u001b[0m \u001b[38;5;66;03m# NOTE Setting initial ryp_rate when using physics.sys_id will not have an impact\u001b[39;00m\n\u001b[1;32m    180\u001b[0m \u001b[38;5;66;03m# Sample initial pos\u001b[39;00m\n",
      "File \u001b[0;32m~/venv/lib/python3.11/site-packages/crazyflow/sim/sim.py:268\u001b[0m, in \u001b[0;36mSim.reset\u001b[0;34m(self, mask)\u001b[0m\n\u001b[1;32m    261\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Reset the simulation to the initial state.\u001b[39;00m\n\u001b[1;32m    262\u001b[0m \n\u001b[1;32m    263\u001b[0m \u001b[38;5;124;03mArgs:\u001b[39;00m\n\u001b[1;32m    264\u001b[0m \u001b[38;5;124;03m    mask: Boolean array of shape (n_worlds, ) that indicates which worlds to reset. If None,\u001b[39;00m\n\u001b[1;32m    265\u001b[0m \u001b[38;5;124;03m        all worlds are reset.\u001b[39;00m\n\u001b[1;32m    266\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    267\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m mask \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m mask\u001b[38;5;241m.\u001b[39mshape \u001b[38;5;241m==\u001b[39m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_worlds,), \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMask shape mismatch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmask\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m--> 268\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_reset\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdefault_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmask\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/venv/lib/python3.11/site-packages/jax/_src/tree_util.py:868\u001b[0m, in \u001b[0;36mregister_pytree_with_keys.<locals>.flatten_func_impl\u001b[0;34m(tree)\u001b[0m\n\u001b[1;32m    867\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mflatten_func_impl\u001b[39m(tree):\n\u001b[0;32m--> 868\u001b[0m   key_children, treedef \u001b[38;5;241m=\u001b[39m \u001b[43mflatten_with_keys\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtree\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    869\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m [c \u001b[38;5;28;01mfor\u001b[39;00m _, c \u001b[38;5;129;01min\u001b[39;00m key_children], treedef\n",
      "File \u001b[0;32m~/venv/lib/python3.11/site-packages/mujoco/mjx/_src/dataclasses.py:73\u001b[0m, in \u001b[0;36mdataclass.<locals>.iterate_clz_with_keys\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m     70\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mto_data\u001b[39m(field, obj):\n\u001b[1;32m     71\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m (jax\u001b[38;5;241m.\u001b[39mtree_util\u001b[38;5;241m.\u001b[39mGetAttrKey(field\u001b[38;5;241m.\u001b[39mname), \u001b[38;5;28mgetattr\u001b[39m(obj, field\u001b[38;5;241m.\u001b[39mname))\n\u001b[0;32m---> 73\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mtuple\u001b[39m(to_data(f, x) \u001b[38;5;28;01mfor\u001b[39;00m f \u001b[38;5;129;01min\u001b[39;00m data_fields)\n\u001b[1;32m     74\u001b[0m meta \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mtuple\u001b[39m(to_meta(f, x) \u001b[38;5;28;01mfor\u001b[39;00m f \u001b[38;5;129;01min\u001b[39;00m meta_fields)\n\u001b[1;32m     75\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m data, meta\n",
      "File \u001b[0;32m~/venv/lib/python3.11/site-packages/mujoco/mjx/_src/dataclasses.py:73\u001b[0m, in \u001b[0;36m<genexpr>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     70\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mto_data\u001b[39m(field, obj):\n\u001b[1;32m     71\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m (jax\u001b[38;5;241m.\u001b[39mtree_util\u001b[38;5;241m.\u001b[39mGetAttrKey(field\u001b[38;5;241m.\u001b[39mname), \u001b[38;5;28mgetattr\u001b[39m(obj, field\u001b[38;5;241m.\u001b[39mname))\n\u001b[0;32m---> 73\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mtuple\u001b[39m(\u001b[43mto_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m f \u001b[38;5;129;01min\u001b[39;00m data_fields)\n\u001b[1;32m     74\u001b[0m meta \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mtuple\u001b[39m(to_meta(f, x) \u001b[38;5;28;01mfor\u001b[39;00m f \u001b[38;5;129;01min\u001b[39;00m meta_fields)\n\u001b[1;32m     75\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m data, meta\n",
      "File \u001b[0;32m~/venv/lib/python3.11/site-packages/mujoco/mjx/_src/dataclasses.py:71\u001b[0m, in \u001b[0;36mdataclass.<locals>.iterate_clz_with_keys.<locals>.to_data\u001b[0;34m(field, obj)\u001b[0m\n\u001b[1;32m     70\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mto_data\u001b[39m(field, obj):\n\u001b[0;32m---> 71\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m (\u001b[43mjax\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtree_util\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mGetAttrKey\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfield\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m, \u001b[38;5;28mgetattr\u001b[39m(obj, field\u001b[38;5;241m.\u001b[39mname))\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# ConfigDict allows us to use convinient dot-based property access: https://github.com/google/ml_collections\n",
    "train_config = ConfigDict(\n",
    "    {\n",
    "        \"n_envs\": 1024,\n",
    "        \"device\": train_device,\n",
    "        \"total_timesteps\": 1_000_000,\n",
    "        \"learning_rate\": 1.5e-3,\n",
    "        \"n_steps\": 16,  # Number of steps per environment per policy rollout\n",
    "        \"gamma\": 0.90,  # Discount factor\n",
    "        \"gae_lambda\": 0.95,  # Lambda for general advantage estimation\n",
    "        \"n_minibatches\": 16,  # Number of mini-batches\n",
    "        \"n_epochs\": 15,\n",
    "        \"norm_adv\": True,\n",
    "        \"clip_coef\": 0.25,\n",
    "        \"clip_vloss\": True,\n",
    "        \"ent_coef\": 0.01,\n",
    "        \"vf_coef\": 0.5,\n",
    "        \"max_grad_norm\": 5.0,\n",
    "        \"target_kl\": None,\n",
    "        \"seed\": 0,\n",
    "        \"n_eval_envs\": 64,\n",
    "        \"n_eval_steps\": 1_000,\n",
    "        \"save_model\": True,\n",
    "        \"eval_interval\": 999_000,\n",
    "        \"lr_decay\": True,\n",
    "    }\n",
    ")\n",
    "\n",
    "set_seeds(train_config.seed)\n",
    "trainer = PPOTrainer(train_config, wandb_log=True)\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\">\n",
    "    <h3>Task 14: Test  your Agent</h3>\n",
    "    <p>\n",
    "        Once training has finished, run the inference using the cell below. \n",
    "    </p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ppo import PPOTester\n",
    "\n",
    "path = Path.cwd() / \"ppo_checkpoint_ex06.pt\"\n",
    "\n",
    "# set render to `True` to see how the agent performs.\n",
    "_ = PPOTester(seed=0, ckpt_path=path, n_episodes=10, render=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
